{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Classification by BertForSequenceClassification**"
      ],
      "metadata": {
        "id": "QQ1ZjoaXufSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JGX_-1IRHMx",
        "outputId": "f8e2de74-ecc3-4685-ba83-8ffbf8db7281"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ltd3KnWi-N0c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the DataFrame\n",
        "file_path = \"/content/drive/MyDrive/Colab_Notebooks2/train_en.tsv\"\n",
        "\n",
        "\n",
        "# Use read_csv with a tab delimiter\n",
        "df = pd.read_csv(file_path, delimiter='\\t')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test start"
      ],
      "metadata": {
        "id": "dtJmaCzLX41A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "ugFr-2DXXwZY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  text = text.lower()  # Convert to lowercase\n",
        "  text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)  # Remove non-alphanumeric characters and spaces\n",
        "  text = re.sub(r\"\\s+\", \" \", text)  # Remove extra spaces\n",
        "  return text\n",
        "\n",
        "# Apply the cleaning function to each sentence\n",
        "df['cleaned_sentence'] = df['sentence'].apply(clean_text)"
      ],
      "metadata": {
        "id": "gdCQw7MGYdRi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "id": "nqJYdNq1VsqM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use preprocessed sentences\n",
        "sentences = df['cleaned_sentence'].values"
      ],
      "metadata": {
        "id": "NhMBff4UVqjp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode all tokenized the sentences\n",
        "input_ids = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    encoded_sentence = tokenizer.encode(sentence)\n",
        "    input_ids.append(encoded_sentence)\n",
        "\n",
        "# example\n",
        "print(sentences[0])\n",
        "print(input_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS3gSPm3WAbD",
        "outputId": "9309868d-26a2-4477-98cf-71adc9baf5f7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gone are the days when they led the world in recessionbusting\n",
            "[101, 2908, 2024, 1996, 2420, 2043, 2027, 2419, 1996, 2088, 1999, 19396, 8286, 3436, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use some utility function from tensorflow\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 150\n",
        "\n",
        "#Padding the input to the max length that is 64\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# example\n",
        "print(sentences[0])\n",
        "print(input_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhhGUf3TWqxe",
        "outputId": "2e869019-fbb6-4a3f-bd24-8b30cb0b6ab6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gone are the days when they led the world in recessionbusting\n",
            "[  101  2908  2024  1996  2420  2043  2027  2419  1996  2088  1999 19396\n",
            "  8286  3436   102     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the attention masks\n",
        "attention_masks = []\n",
        "for i in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in i]\n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "print(attention_masks[0])\n",
        "print(attention_masks[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYbEpUm3XAIt",
        "outputId": "b44778b8-3910-4260-fd82-dfbfa79c60ff"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckqv2MMtXKWX",
        "outputId": "3caac154-4d09-44ec-b496-c6090307df7a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to binary values of 0 and 1\n",
        "label_mapping = {'OBJ': 0, 'SUBJ': 1}\n",
        "\n",
        "# Use map function to replace values in the \"label\" column\n",
        "df['labelBinary'] = df['label'].map(label_mapping)\n",
        "\n",
        "labels = df['labelBinary'].values\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTXCN4DIakJw",
        "outputId": "2a719989-bdea-4fd4-fbbf-7284eb1b0ec8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0\n",
            " 1 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0\n",
            " 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0\n",
            " 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0\n",
            " 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Create a TensorDataset\n",
        "dataset = TensorDataset(torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(labels))\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "O0lPyAnxacm2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.tensor(input_ids).shape)\n",
        "print(torch.tensor(attention_masks).shape)\n",
        "print(torch.tensor(labels).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnFGrqKUy-cJ",
        "outputId": "f9984c28-e19d-4b6f-aa69-82c67150f82e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([830, 150])\n",
            "torch.Size([830, 150])\n",
            "torch.Size([830])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training parameters\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6BbNnOwXnJC",
        "outputId": "ffb72966-9c1c-49f2-e3d4-fecdfed6198a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(2):  # Adjust the number of epochs as needed\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3NIDdAMPnaU",
        "outputId": "2e6139bf-fa48-46dd-9785-83c9ccc7af0c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.3198506534099579\n",
            "Epoch 2, Loss: 0.09345953911542892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of BertForSequenceClassification"
      ],
      "metadata": {
        "id": "lCO8CkG_oWp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load test data\n",
        "file_path2 = \"/content/drive/MyDrive/Colab_Notebooks2/dev_test_en.tsv\"\n",
        "\n",
        "\n",
        "# Use read_csv with a tab delimiter\n",
        "df_test = pd.read_csv(file_path2, delimiter='\\t')\n",
        "\n",
        "# Load the DataFrame to confirm it's loaded correctly\n",
        "print(df_test.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NmxmjBfmKH9",
        "outputId": "d44477b5-3192-4145-9984-0c58ba6739e8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            sentence_id  \\\n",
            "0  8745d4da-91c9-4538-acee-b0e7b1c413fd   \n",
            "1  43de04ad-d0ac-4852-9b4e-cf0bca066188   \n",
            "2  e00b66ee-720a-47e3-a0fb-0e2445b89af6   \n",
            "3  0b95d635-f821-45dd-9f33-b05d63629195   \n",
            "4  5ba3117b-3ef9-4815-acb4-a263d3c816bc   \n",
            "\n",
            "                                            sentence label  \n",
            "0  Who will redistribute the hoarded wealth that ...  SUBJ  \n",
            "1  What we don’t need is the indiscriminate influ...  SUBJ  \n",
            "2  The Social Distance Between Us shows every sig...   OBJ  \n",
            "3  History shows that McCarthy and McConnell, lik...   OBJ  \n",
            "4  So while it’s not hard to reach a banal point ...  SUBJ  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the cleaning function to each sentence\n",
        "df_test['cleaned_sentence'] = df_test['sentence'].apply(clean_text)\n",
        "sentences_test = df_test['cleaned_sentence'].values\n",
        "\n",
        "# Use map function to replace values in the \"label\" column\n",
        "label_mapping = {'OBJ': 0, 'SUBJ': 1}\n",
        "df_test['labelBinary'] = df_test['label'].map(label_mapping)\n",
        "labels_test = df_test['labelBinary'].values\n"
      ],
      "metadata": {
        "id": "MiJ_fvcum9sh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import XLMRobertaTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# encode all tokenized the sentences\n",
        "input_ids_test = []\n",
        "\n",
        "for sentence in sentences_test:\n",
        "    encoded_sentence = tokenizer.encode(sentence)\n",
        "    input_ids_test.append(encoded_sentence)\n",
        "\n",
        "\n",
        "MAX_LEN = 150\n",
        "#Padding the input to the max length that is 64\n",
        "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "\n",
        "# Creating the attention masks\n",
        "attention_masks_test = []\n",
        "for i in input_ids_test:\n",
        "    att_mask = [int(token_id > 0) for token_id in i]\n",
        "    attention_masks_test.append(att_mask)"
      ],
      "metadata": {
        "id": "a36J9-m2uF5Y"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TensorDataset\n",
        "dataset_test = TensorDataset(torch.tensor(input_ids_test), torch.tensor(attention_masks_test), torch.tensor(labels_test))\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "zNEanQMdrK7E"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.tensor(input_ids_test).shape)\n",
        "print(torch.tensor(attention_masks_test).shape)\n",
        "print(torch.tensor(labels_test).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzQvnoMvtdKr",
        "outputId": "e16a5672-6c01-4997-c54a-ffa37d0813a6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([243, 150])\n",
            "torch.Size([243, 150])\n",
            "torch.Size([243])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables to store predictions and true labels\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "with torch.no_grad():\n",
        "    for batch in dataloader_test:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "classification_rep = classification_report(true_labels, predictions)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBBElLIfsejY",
        "outputId": "0cf7b052-6f6b-4d28-e162-dfa9974336e8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7654320987654321\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.89      0.78       116\n",
            "           1       0.86      0.65      0.74       127\n",
            "\n",
            "    accuracy                           0.77       243\n",
            "   macro avg       0.78      0.77      0.76       243\n",
            "weighted avg       0.79      0.77      0.76       243\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification by XLMRobertaForSequenceClassification**"
      ],
      "metadata": {
        "id": "brmYUz-uZ0Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the DataFrame\n",
        "file_path = \"/content/drive/MyDrive/Colab_Notebooks2/train_en.tsv\"\n",
        "\n",
        "# Use read_csv with a tab delimiter\n",
        "df = pd.read_csv(file_path, delimiter='\\t')\n"
      ],
      "metadata": {
        "id": "31Z5nOVTdWKL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  text = text.lower()  # Convert to lowercase\n",
        "  text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)  # Remove non-alphanumeric characters and spaces\n",
        "  text = re.sub(r\"\\s+\", \" \", text)  # Remove extra spaces\n",
        "  return text\n",
        "\n",
        "# Apply the cleaning function to each sentence\n",
        "df['cleaned_sentence'] = df['sentence'].apply(clean_text)"
      ],
      "metadata": {
        "id": "S7OWBf-Fdh31"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW\n",
        "\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', language='en', do_lower_case=True)"
      ],
      "metadata": {
        "id": "knCzavvHaCuR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use preprocessed sentences\n",
        "sentences = df['cleaned_sentence'].values\n",
        "\n",
        "# encode all tokenized the sentences\n",
        "input_ids = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    encoded_sentence = tokenizer.encode(sentence)\n",
        "    input_ids.append(encoded_sentence)\n",
        "\n",
        "# We will use some utility function from tensorflow\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 150\n",
        "\n",
        "#Padding the input to the max length that is 64\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Creating the attention masks\n",
        "attention_masks = []\n",
        "for i in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in i]\n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW\n",
        "\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
        "          #'xlm-roberta-large'\n",
        "         'xlm-roberta-base',\n",
        "         num_labels=2,\n",
        "         output_attentions = False,\n",
        "         output_hidden_states = False,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPxlJLTra1Di",
        "outputId": "0e6b0371-84f2-4618-8df7-4241e6b3dffb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to binary values of 0 and 1\n",
        "label_mapping = {'OBJ': 0, 'SUBJ': 1}\n",
        "\n",
        "# Use map function to replace values in the \"label\" column\n",
        "df['labelBinary'] = df['label'].map(label_mapping)\n",
        "\n",
        "labels = df['labelBinary'].values\n"
      ],
      "metadata": {
        "id": "f4n42ad7a2rW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Create a TensorDataset\n",
        "dataset = TensorDataset(torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(labels))\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Set up training parameters\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvxRRYyQbqzr",
        "outputId": "9a0a2446-5b94-427a-e37e-3c734c3948b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaForSequenceClassification(\n",
              "  (roberta): XLMRobertaModel(\n",
              "    (embeddings): XLMRobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): XLMRobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): XLMRobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(2):  # Adjust the number of epochs as needed\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqMtxSvWbxAp",
        "outputId": "8eaa6b78-74b4-4292-ef29-4df47a6d3645"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.5407558083534241\n",
            "Epoch 2, Loss: 0.5217330455780029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of XLMRobertaForSequenceClassification"
      ],
      "metadata": {
        "id": "aeG3VAEwb5Cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# load test data\n",
        "file_path2 = \"/content/drive/MyDrive/Colab_Notebooks2/dev_test_en.tsv\"\n",
        "\n",
        "\n",
        "# Use read_csv with a tab delimiter\n",
        "df_test = pd.read_csv(file_path2, delimiter='\\t')\n",
        "\n",
        "# Load the DataFrame to confirm it's loaded correctly\n",
        "print(df_test.head())\n",
        "\n",
        "\n",
        "# Apply the cleaning function to each sentence\n",
        "df_test['cleaned_sentence'] = df_test['sentence'].apply(clean_text)\n",
        "sentences_test = df_test['cleaned_sentence'].values\n",
        "\n",
        "# Use map function to replace values in the \"label\" column\n",
        "label_mapping = {'OBJ': 0, 'SUBJ': 1}\n",
        "df_test['labelBinary'] = df_test['label'].map(label_mapping)\n",
        "labels_test = df_test['labelBinary'].values\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import XLMRobertaTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# encode all tokenized the sentences\n",
        "input_ids_test = []\n",
        "\n",
        "for sentence in sentences_test:\n",
        "    encoded_sentence = tokenizer.encode(sentence)\n",
        "    input_ids_test.append(encoded_sentence)\n",
        "\n",
        "\n",
        "MAX_LEN = 150\n",
        "#Padding the input to the max length that is 64\n",
        "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "\n",
        "# Creating the attention masks\n",
        "attention_masks_test = []\n",
        "for i in input_ids_test:\n",
        "    att_mask = [int(token_id > 0) for token_id in i]\n",
        "    attention_masks_test.append(att_mask)\n",
        "\n",
        "# Create a TensorDataset\n",
        "dataset_test = TensorDataset(torch.tensor(input_ids_test), torch.tensor(attention_masks_test), torch.tensor(labels_test))\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=8, shuffle=True)\n",
        "\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables to store predictions and true labels\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "with torch.no_grad():\n",
        "    for batch in dataloader_test:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "classification_rep = classification_report(true_labels, predictions)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69zO7Eulb3Em",
        "outputId": "0726bd33-ca0e-4350-e013-68329ec86625"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            sentence_id  \\\n",
            "0  8745d4da-91c9-4538-acee-b0e7b1c413fd   \n",
            "1  43de04ad-d0ac-4852-9b4e-cf0bca066188   \n",
            "2  e00b66ee-720a-47e3-a0fb-0e2445b89af6   \n",
            "3  0b95d635-f821-45dd-9f33-b05d63629195   \n",
            "4  5ba3117b-3ef9-4815-acb4-a263d3c816bc   \n",
            "\n",
            "                                            sentence label  \n",
            "0  Who will redistribute the hoarded wealth that ...  SUBJ  \n",
            "1  What we don’t need is the indiscriminate influ...  SUBJ  \n",
            "2  The Social Distance Between Us shows every sig...   OBJ  \n",
            "3  History shows that McCarthy and McConnell, lik...   OBJ  \n",
            "4  So while it’s not hard to reach a banal point ...  SUBJ  \n",
            "Accuracy: 0.4773662551440329\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      1.00      0.65       116\n",
            "           1       0.00      0.00      0.00       127\n",
            "\n",
            "    accuracy                           0.48       243\n",
            "   macro avg       0.24      0.50      0.32       243\n",
            "weighted avg       0.23      0.48      0.31       243\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}